#!/usr/bin/env python3
"""QoS Compliance Analysis Tool for Thesis Experiments.

This utility processes the structured handover logs emitted by the NEF
handover engine and produces per-service QoS compliance statistics,
violation breakdowns, timeline visualisations, and ML vs A3 comparison
summaries. It is designed to run against the artefacts generated by
``scripts/run_thesis_experiment.sh`` but can also analyse standalone log
files if required.

Usage examples
--------------

Analyse a full experiment directory (preferred)::

    python scripts/analyze_qos_compliance.py \
        --results-dir thesis_results/baseline_experiment

Analyse explicit log and metric files::

    python scripts/analyze_qos_compliance.py \
        --ml-log path/to/ml_mode_docker.log \
        --ml-metrics path/to/ml_mode_metrics.json \
        --a3-metrics path/to/a3_mode_metrics.json \
        --output output/qos_analysis

Author: Thesis Project
Date: November 2025
"""

from __future__ import annotations

import argparse
import json
import sys
from collections import Counter, defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import matplotlib

matplotlib.use("Agg")
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns


# Repository-relative imports -------------------------------------------------

REPO_ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(REPO_ROOT))
# Also make the service package available (it lives under 5g-network-optimization/services)
sys.path.insert(0, str(REPO_ROOT / "5g-network-optimization"))

from services.logging_config import configure_logging  # type: ignore

import logging

logger = logging.getLogger(__name__)


# Helper utilities ------------------------------------------------------------


def _load_json(path: Path) -> Dict[str, Any]:
    if not path or not path.exists():
        return {}
    try:
        with path.open() as handle:
            return json.load(handle)
    except json.JSONDecodeError as exc:  # noqa: BLE001
        logger.error("Failed to parse JSON file %s: %s", path, exc)
        return {}


def _coerce_float(value: Any, fallback: float = 0.0) -> float:
    try:
        return float(value)
    except (TypeError, ValueError):
        return float(fallback)


def _extract_prom_scalar(payload: Dict[str, Any]) -> float:
    data = payload.get("data", {}) if isinstance(payload, dict) else {}
    result = data.get("result") if isinstance(data, dict) else []
    total = 0.0
    if isinstance(result, list):
        for entry in result:
            if not isinstance(entry, dict):
                continue
            value = entry.get("value")
            if isinstance(value, (list, tuple)) and len(value) >= 2:
                total += _coerce_float(value[1], 0.0)
    return total


def _extract_prom_vector(payload: Dict[str, Any], label: str) -> Dict[str, float]:
    data = payload.get("data", {}) if isinstance(payload, dict) else {}
    result = data.get("result") if isinstance(data, dict) else []
    output: Dict[str, float] = {}
    if isinstance(result, list):
        for entry in result:
            if not isinstance(entry, dict):
                continue
            metric = entry.get("metric", {})
            key = metric.get(label, "unknown") if isinstance(metric, dict) else "unknown"
            value = entry.get("value")
            if isinstance(value, (list, tuple)) and len(value) >= 2:
                output[key] = _coerce_float(value[1], 0.0)
    return output


# QoS log parsing -------------------------------------------------------------


class QoSLogAnalyzer:
    """Parse structured QoS handover logs produced by the NEF emulator."""

    HANDOVER_MARKERS = ("HANDOVER_APPLIED:", "HANDOVER_DECISION:")

    def __init__(self, log_path: Path):
        self.log_path = log_path
        self.records: List[Dict[str, Any]] = []
        self.violations: List[Dict[str, Any]] = []
        self._raw_entries: Dict[Tuple[Optional[str], Optional[str]], Dict[str, Any]] = {}

    def parse(self) -> None:
        self.records.clear()
        self.violations.clear()
        self._raw_entries.clear()

        if not self.log_path.exists():
            logger.warning("Log file not found: %s", self.log_path)
            return

        with self.log_path.open("r", encoding="utf-8", errors="ignore") as handle:
            for line in handle:
                marker, payload = self._extract_payload(line)
                if not payload:
                    continue
                try:
                    data = json.loads(payload)
                except json.JSONDecodeError:
                    continue

                key = (data.get("timestamp"), data.get("ue_id"))

                if marker == "HANDOVER_APPLIED:" or key not in self._raw_entries:
                    self._raw_entries[key] = data
                else:
                    self._raw_entries[key].update(data)

        for entry in self._raw_entries.values():
            record = self._normalise_entry(entry)
            if record:
                self.records.append(record)

    def _extract_payload(self, line: str) -> Tuple[Optional[str], Optional[str]]:
        for marker in self.HANDOVER_MARKERS:
            idx = line.find(marker)
            if idx != -1:
                payload = line[idx + len(marker) :].strip()
                brace = payload.find("{")
                if brace > 0:
                    payload = payload[brace:]
                return marker, payload
        return None, None

    def _normalise_entry(self, entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        qos = entry.get("qos_compliance") if isinstance(entry, dict) else None
        if not isinstance(qos, dict):
            return None
        if qos.get("checked") is False and not qos.get("metrics"):
            return None

        service_type = qos.get("service_type")
        details = qos.get("details") if isinstance(qos.get("details"), dict) else {}
        if not service_type:
            service_type = details.get("service_type", "unknown")

        timestamp = entry.get("timestamp") or entry.get("time")
        record: Dict[str, Any] = {
            "timestamp": timestamp,
            "service_type": service_type or "unknown",
            "passed": bool(qos.get("passed", qos.get("service_priority_ok", False))),
            "confidence_ok": bool(qos.get("confidence_ok", True)),
            "required_confidence": _coerce_float(qos.get("required_confidence"), 0.0),
            "observed_confidence": _coerce_float(qos.get("observed_confidence"), 0.0),
            "fallback": bool(entry.get("fallback_to_a3")),
            "violations": qos.get("violations") if isinstance(qos.get("violations"), list) else [],
        }

        metrics = qos.get("metrics") if isinstance(qos.get("metrics"), dict) else {}
        for metric_name, metric_data in metrics.items():
            if not isinstance(metric_data, dict):
                continue
            base = metric_name.lower()
            record[f"{base}_passed"] = bool(metric_data.get("passed", True))
            if metric_name == "reliability":
                record[f"{base}_required"] = _coerce_float(metric_data.get("required_loss_max"), 0.0)
                record[f"{base}_observed"] = _coerce_float(metric_data.get("observed_loss"), 0.0)
            else:
                record[f"{base}_required"] = _coerce_float(metric_data.get("required"), 0.0)
                record[f"{base}_observed"] = _coerce_float(metric_data.get("observed"), 0.0)
            record[f"{base}_delta"] = _coerce_float(metric_data.get("delta"), 0.0)

        if record["violations"]:
            for violation in record["violations"]:
                if not isinstance(violation, dict):
                    continue
                self.violations.append(
                    {
                        "timestamp": timestamp,
                        "service_type": record["service_type"],
                        "metric": violation.get("metric", "unknown"),
                        "required": _coerce_float(violation.get("required"), 0.0),
                        "observed": _coerce_float(violation.get("observed"), 0.0),
                        "delta": _coerce_float(violation.get("delta"), 0.0),
                    }
                )

        return record

    def to_dataframe(self) -> pd.DataFrame:
        return pd.DataFrame(self.records)

    def violations_dataframe(self) -> pd.DataFrame:
        return pd.DataFrame(self.violations)


# Analysis routines -----------------------------------------------------------


class QoSComplianceReport:
    """Aggregate statistics derived from QoS handover records."""

    def __init__(self, df: pd.DataFrame, violations: pd.DataFrame):
        self.df = df.copy()
        self.violations = violations.copy()
        if not self.df.empty:
            self.df["timestamp"] = pd.to_datetime(self.df["timestamp"], utc=True, errors="coerce")
            self.df = self.df.dropna(subset=["timestamp"]).sort_values("timestamp")
        if not self.violations.empty:
            self.violations["timestamp"] = pd.to_datetime(
                self.violations["timestamp"], utc=True, errors="coerce"
            )
            self.violations = self.violations.dropna(subset=["timestamp"])

    def per_service_summary(self) -> pd.DataFrame:
        if self.df.empty:
            return pd.DataFrame()

        grouped = self.df.groupby("service_type")

        summary = grouped["passed"].agg(
            total="count",
            passed="sum",
        )
        summary["failed"] = summary["total"] - summary["passed"]
        summary["compliance_rate"] = summary["passed"] / summary["total"].where(summary["total"] > 0, 1)

        if "confidence_ok" in self.df.columns:
            summary["confidence_ok_rate"] = grouped["confidence_ok"].mean()
        if "fallback" in self.df.columns:
            summary["fallback_rate"] = grouped["fallback"].mean()

        metric_columns = defaultdict(dict)
        for metric in ("latency", "throughput", "jitter", "reliability"):
            delta_col = f"{metric}_delta"
            obs_col = f"{metric}_observed"
            req_col = f"{metric}_required"
            if delta_col in self.df.columns:
                metric_columns[f"avg_{metric}_delta"] = grouped[delta_col].mean()
            if obs_col in self.df.columns:
                metric_columns[f"avg_{metric}_observed"] = grouped[obs_col].mean()
            if req_col in self.df.columns:
                metric_columns[f"avg_{metric}_required"] = grouped[req_col].mean()

        for column_name, series in metric_columns.items():
            summary[column_name] = series

        summary = summary.reset_index()
        return summary.sort_values("service_type")

    def violation_summary(self) -> pd.DataFrame:
        if self.violations.empty:
            return pd.DataFrame()

        grouped = self.violations.groupby(["service_type", "metric"])
        summary = grouped.size().reset_index(name="count")
        summary = summary.sort_values("count", ascending=False)
        return summary

    def top_violation_reasons(self, top_n: int = 5) -> List[Tuple[str, int]]:
        if self.violations.empty:
            return []
        counts = Counter(self.violations["metric"].tolist())
        return counts.most_common(top_n)

    def compliance_timeline(self, bin_seconds: int) -> pd.DataFrame:
        if self.df.empty:
            return pd.DataFrame()

        df = self.df.set_index("timestamp")
        rule = f"{max(bin_seconds, 10)}S"

        overall = df.resample(rule)["passed"].mean().rename("overall_compliance")

        per_service = (
            df.groupby("service_type")["passed"]
            .resample(rule)
            .mean()
            .unstack(level=0)
        )

        combined = pd.concat([overall, per_service], axis=1)
        combined.index.name = "timestamp"
        return combined.dropna(how="all")


class MetricsComparator:
    """Summarise QoS metrics exported from Prometheus snapshots."""

    def __init__(self, ml_metrics: Dict[str, Any], a3_metrics: Dict[str, Any]):
        self.ml_metrics = ml_metrics
        self.a3_metrics = a3_metrics

    def _get_metric_payload(self, metrics: Dict[str, Any], key: str) -> Dict[str, Any]:
        value = metrics.get(key)
        return value if isinstance(value, dict) else {}

    def _load_metrics_map(self, raw: Dict[str, Any]) -> Dict[str, Any]:
        return raw.get("metrics", raw) if isinstance(raw, dict) else {}

    def compare(self) -> Dict[str, Any]:
        ml_raw = self._load_metrics_map(self.ml_metrics)
        a3_raw = self._load_metrics_map(self.a3_metrics)

        ml_pass = _extract_prom_vector(self._get_metric_payload(ml_raw, "qos_pass_by_service"), "service_type")
        ml_fail = _extract_prom_vector(self._get_metric_payload(ml_raw, "qos_fail_by_service"), "service_type")

        ml_total_pass = sum(ml_pass.values())
        ml_total_fail = sum(ml_fail.values())
        if ml_total_pass == 0 and ml_total_fail == 0:
            ml_total_pass = _extract_prom_scalar(self._get_metric_payload(ml_raw, "qos_compliance_ok"))
            ml_total_fail = _extract_prom_scalar(self._get_metric_payload(ml_raw, "qos_compliance_failed"))

        a3_pass = _extract_prom_scalar(self._get_metric_payload(a3_raw, "qos_compliance_ok"))
        a3_fail = _extract_prom_scalar(self._get_metric_payload(a3_raw, "qos_compliance_failed"))

        ml_rate = ml_total_pass / (ml_total_pass + ml_total_fail) if (ml_total_pass + ml_total_fail) > 0 else 0.0
        a3_rate = a3_pass / (a3_pass + a3_fail) if (a3_pass + a3_fail) > 0 else 0.0

        violation_vector = _extract_prom_vector(self._get_metric_payload(ml_raw, "qos_violations_by_metric"), "metric")
        adaptive_conf = _extract_prom_vector(self._get_metric_payload(ml_raw, "adaptive_confidence"), "service_type")

        return {
            "ml": {
                "pass_total": ml_total_pass,
                "fail_total": ml_total_fail,
                "compliance_rate": ml_rate,
                "by_service": {svc: {
                    "passed": ml_pass.get(svc, 0.0),
                    "failed": ml_fail.get(svc, 0.0),
                    "compliance_rate": (
                        ml_pass.get(svc, 0.0)
                        / (ml_pass.get(svc, 0.0) + ml_fail.get(svc, 0.0))
                        if (ml_pass.get(svc, 0.0) + ml_fail.get(svc, 0.0)) > 0
                        else 0.0
                    ),
                    "adaptive_confidence": adaptive_conf.get(svc),
                } for svc in set(list(ml_pass.keys()) + list(ml_fail.keys()) + list(adaptive_conf.keys()))},
                "violations_by_metric": violation_vector,
            },
            "a3": {
                "pass_total": a3_pass,
                "fail_total": a3_fail,
                "compliance_rate": a3_rate,
            },
            "delta": {
                "compliance_rate_delta": ml_rate - a3_rate,
                "additional_passes": ml_total_pass - a3_pass,
                "reduced_failures": a3_fail - ml_total_fail,
            },
        }


# Visualisation ---------------------------------------------------------------


def _plot_compliance_timeline(timeline: pd.DataFrame, output_dir: Path) -> Optional[Path]:
    if timeline.empty:
        return None

    sns.set_style("whitegrid")
    plt.figure(figsize=(14, 7))

    # Overall compliance shown first for clarity.
    if "overall_compliance" in timeline.columns:
        plt.plot(
            timeline.index,
            timeline["overall_compliance"] * 100,
            label="Overall",
            color="#1E88E5",
            linewidth=2.5,
        )

    for column in timeline.columns:
        if column == "overall_compliance":
            continue
        plt.plot(
            timeline.index,
            timeline[column] * 100,
            label=column.upper(),
            linewidth=2.0,
        )

    plt.axhline(95, color="#D81B60", linestyle="--", linewidth=1.5, label="Target 95%")
    plt.ylabel("Compliance Rate (%)", fontsize=12, fontweight="bold")
    plt.xlabel("Time", fontsize=12, fontweight="bold")
    plt.title("QoS Compliance Timeline", fontsize=14, fontweight="bold")
    plt.ylim(0, 105)
    plt.legend(loc="lower right")
    plt.tight_layout()

    output_dir.mkdir(parents=True, exist_ok=True)
    plot_path = output_dir / "qos_compliance_timeline.png"
    plt.savefig(plot_path, dpi=300, bbox_inches="tight")
    plt.close()
    return plot_path


def _plot_violation_breakdown(summary: pd.DataFrame, output_dir: Path) -> Optional[Path]:
    if summary.empty:
        return None

    pivot = summary.pivot_table(index="metric", columns="service_type", values="count", aggfunc="sum", fill_value=0)

    sns.set_style("whitegrid")
    fig, ax = plt.subplots(figsize=(12, 7))
    pivot.plot(kind="bar", colormap="YlOrRd", ax=ax, edgecolor="black")
    ax.set_ylabel("Violations", fontsize=12, fontweight="bold")
    ax.set_xlabel("Metric", fontsize=12, fontweight="bold")
    ax.set_title("QoS Violations by Metric and Service", fontsize=14, fontweight="bold")
    ax.legend(title="Service Type")
    fig.tight_layout()

    output_dir.mkdir(parents=True, exist_ok=True)
    plot_path = output_dir / "qos_violation_breakdown.png"
    fig.savefig(plot_path, dpi=300, bbox_inches="tight")
    plt.close(fig)
    return plot_path


# CLI ------------------------------------------------------------------------


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="Analyse QoS compliance logs and metrics",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python analyze_qos_compliance.py --results-dir thesis_results/run_01
  python analyze_qos_compliance.py --ml-log logs/ml.log --ml-metrics metrics.json --output output/qos
        """,
    )

    parser.add_argument("--results-dir", type=str, help="Experiment directory produced by the thesis runner")
    parser.add_argument("--ml-log", type=str, help="Path to ML mode log file")
    parser.add_argument("--a3-log", type=str, help="Path to A3 mode log file (optional)")
    parser.add_argument("--ml-metrics", type=str, help="Path to ML metrics JSON snapshot")
    parser.add_argument("--a3-metrics", type=str, help="Path to A3 metrics JSON snapshot")
    parser.add_argument("--output", type=str, help="Output directory for analysis artefacts")
    parser.add_argument("--bin-size", type=int, default=60, help="Timeline bin size in seconds (default: 60)")
    parser.add_argument("--no-plots", action="store_true", help="Skip plot generation")

    return parser.parse_args()


def _resolve_paths(args: argparse.Namespace) -> Dict[str, Optional[Path]]:
    base = Path(args.results_dir).resolve() if args.results_dir else None
    resolved = {
        "ml_log": Path(args.ml_log).resolve() if args.ml_log else (base / "logs/ml_mode_docker.log" if base else None),
        "a3_log": Path(args.a3_log).resolve() if args.a3_log else (base / "logs/a3_mode_docker.log" if base else None),
        "ml_metrics": Path(args.ml_metrics).resolve() if args.ml_metrics else (base / "metrics/ml_mode_metrics.json" if base else None),
        "a3_metrics": Path(args.a3_metrics).resolve() if args.a3_metrics else (base / "metrics/a3_mode_metrics.json" if base else None),
        "output": Path(args.output).resolve() if args.output else (base / "qos_analysis" if base else Path("output/qos_analysis")),
    }
    return resolved


def main() -> int:
    configure_logging()
    args = parse_args()
    paths = _resolve_paths(args)

    output_dir = paths["output"]
    assert output_dir is not None
    output_dir.mkdir(parents=True, exist_ok=True)

    report_sections: List[str] = []
    report: Optional[QoSComplianceReport] = None
    df = pd.DataFrame()
    violation_summary = pd.DataFrame()

    # ------------------------------------------------------------------
    # Parse ML QoS log
    # ------------------------------------------------------------------
    analyzer: Optional[QoSLogAnalyzer] = None
    ml_log_path = paths.get("ml_log")
    if ml_log_path:  # type: ignore[truthy-function]
        analyzer = QoSLogAnalyzer(ml_log_path)
        analyzer.parse()
        df = analyzer.to_dataframe()
        violations_df = analyzer.violations_dataframe()

        if df.empty:
            logger.warning("No QoS records parsed from %s", ml_log_path)
        else:
            report_sections.append(f"Parsed {len(df)} QoS handover records from {ml_log_path.name}.")

        report = QoSComplianceReport(df, violations_df)
        per_service = report.per_service_summary()
        violation_summary = report.violation_summary()

        if not per_service.empty:
            per_service_csv = output_dir / "per_service_compliance.csv"
            per_service.to_csv(per_service_csv, index=False)
            report_sections.append(f"Per-service compliance written to {per_service_csv.name}.")

        if not violation_summary.empty:
            violations_csv = output_dir / "violation_summary.csv"
            violation_summary.to_csv(violations_csv, index=False)
            report_sections.append(f"Violation breakdown written to {violations_csv.name}.")

        timeline_path = None
        violation_plot_path = None

        if not args.no_plots:
            timeline = report.compliance_timeline(args.bin_size)
            timeline_csv = output_dir / "compliance_timeline.csv"
            if not timeline.empty:
                timeline.to_csv(timeline_csv)
                report_sections.append(f"Timeline data saved to {timeline_csv.name}.")
                timeline_path = _plot_compliance_timeline(timeline, output_dir)
            violation_plot_path = _plot_violation_breakdown(violation_summary, output_dir)

        # Persist consolidated JSON summary for downstream tooling.
        summary_payload: Dict[str, Any] = {
            "generated_at": datetime.now(timezone.utc).isoformat(),
            "per_service": per_service.to_dict(orient="records") if not per_service.empty else [],
            "violations": violation_summary.to_dict(orient="records") if not violation_summary.empty else [],
            "top_violation_metrics": report.top_violation_reasons(),
            "source_log": str(ml_log_path),
            "plots": {
                "timeline": str(timeline_path) if timeline_path else None,
                "violations": str(violation_plot_path) if violation_plot_path else None,
            },
        }
        with (output_dir / "qos_compliance_summary.json").open("w") as handle:
            json.dump(summary_payload, handle, indent=2, default=str)
    else:
        report_sections.append("No ML log supplied; skipping log-based analysis.")

    # ------------------------------------------------------------------
    # Compare ML vs A3 metrics when provided
    # ------------------------------------------------------------------
    ml_metrics_path = paths.get("ml_metrics")
    a3_metrics_path = paths.get("a3_metrics")
    if ml_metrics_path and ml_metrics_path.exists() and a3_metrics_path and a3_metrics_path.exists():
        ml_metrics = _load_json(ml_metrics_path)
        a3_metrics = _load_json(a3_metrics_path)
        comparator = MetricsComparator(ml_metrics, a3_metrics)
        comparison = comparator.compare()
        comparison_path = output_dir / "ml_vs_a3_qos_summary.json"
        with comparison_path.open("w") as handle:
            json.dump(comparison, handle, indent=2)
        report_sections.append(f"ML vs A3 metrics comparison saved to {comparison_path.name}.")
    else:
        report_sections.append("Prometheus metric snapshots incomplete; skipping ML vs A3 comparison.")

    if analyzer and analyzer.records:
        logger.info("Parsed %d QoS records", len(analyzer.records))
    logger.info("Analysis output directory: %s", output_dir)

    print("\nQoS Compliance Analysis Report")
    print("=" * 34)
    for section in report_sections:
        print(f"- {section}")

    if report:
        top_metrics = report.top_violation_reasons()
        if top_metrics:
            print("\nMost common violation metrics:")
            for metric, count in top_metrics:
                print(f"  • {metric}: {count}")

    print(f"\n✅ Analysis complete. Artefacts available in {output_dir}\n")
    return 0


if __name__ == "__main__":
    try:
        sys.exit(main())
    except KeyboardInterrupt:
        logger.info("Analysis interrupted by user")
        sys.exit(1)
    except Exception as exc:  # noqa: BLE001
        logger.error("Analysis failed: %s", exc, exc_info=True)
        sys.exit(1)
